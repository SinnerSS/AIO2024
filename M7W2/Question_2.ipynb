{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FxdCR0RYbCgW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.resnet import ResNet18_Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(\"Path to dataset files: \", DATA_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFdPlJQKbnwV",
        "outputId": "4f1f2255-c58e-421e-ea13-ea1fd97751cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/andrewmvd/dog-and-cat-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.03G/1.03G [00:06<00:00, 166MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files:  /root/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_images_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) <= 1:\n",
        "                    valid_images_files.append(img_name)\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"Image {img_name} has multiple objects and will be excluded from the dataset\"\n",
        "                    )\n",
        "        return valid_images_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall('object'):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        annotation_name = os.path.splitext(img_name)[0] + '.xml'\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        label, bbox = self.parse_annotation(annotation_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, bbox\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        img_width = int(root.find('size').find('width').text)\n",
        "        img_height = int(root.find('size').find('height').text)\n",
        "\n",
        "        label = None\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            if label is None:\n",
        "                label = name\n",
        "\n",
        "                xmin = int(obj.find('bndbox').find('xmin').text)\n",
        "                ymin = int(obj.find('bndbox').find('ymin').text)\n",
        "                xmax = int(obj.find('bndbox').find('xmax').text)\n",
        "                ymax = int(obj.find('bndbox').find('ymax').text)\n",
        "\n",
        "                bbox = [\n",
        "                    xmin / img_width,\n",
        "                    ymin / img_height,\n",
        "                    xmax / img_width,\n",
        "                    ymax / img_height,\n",
        "                ]\n",
        "\n",
        "        label_num = 0 if label == 'cat' else 1 if label == 'dog' else -1\n",
        "\n",
        "        return label_num, torch.tensor(bbox, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "jXGHTDtIbuYb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_dir = os.path.join(DATA_DIR, 'annotations')\n",
        "image_dir = os.path.join(DATA_DIR, 'images')\n",
        "\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name': image_files})\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.486, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "\n",
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw8q51DSceCJ",
        "outputId": "174b5cb1-6576-4ce7-9b82-999d99985d84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Cats_Test736.png has multiple objects and will be excluded from the dataset\n",
            "Image Cats_Test736.png has multiple objects and will be excluded from the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoHeadedModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(TwoHeadedModel, self).__init__()\n",
        "\n",
        "        self.base_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.num_ftrs = self.base_model.fc.in_features\n",
        "\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Linear(self.num_ftrs, num_classes)\n",
        "\n",
        "        self.regressor = nn.Linear(self.num_ftrs, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        class_logits = self.classifier(x)\n",
        "        bbox_cords = torch.sigmoid(self.regressor(x))\n",
        "\n",
        "        return class_logits, bbox_cords"
      ],
      "metadata": {
        "id": "G03Orb-UcfN4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoHeadedModel()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_bbox = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnNpnEizdszx",
        "outputId": "8fea867a-bbcf-4705-8ea2-0ea2f194051f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 111MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets, bboxs) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        bboxs = bboxs.to(device)\n",
        "\n",
        "        score, bboxs_pred = model(data)\n",
        "        loss_class = criterion_class(score, targets)\n",
        "        loss_bbox = criterion_bbox(bboxs_pred, bboxs)\n",
        "        loss = loss_class + loss_bbox\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss_bbox = 0\n",
        "        total_samples = 0\n",
        "        for data, targets, bboxs in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            bboxs = bboxs.to(device)\n",
        "\n",
        "            scores, bboxs_pred = model(data)\n",
        "            _, predictions = scores.max(1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            total_loss_bbox += criterion_bbox(bboxs_pred, bboxs).item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "\n",
        "        avg_loss_bbox = total_loss_bbox / total_samples\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {correct/total*100:.2f}%, Avg. Bbox Loss: {avg_loss_bbox:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-9lA2VLd9U2",
        "outputId": "75d8d5a7-ac9c-4b93-93c0-b6ebb16a5728"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Accuracy: 93.50%, Avg. Bbox Loss: 0.0180\n",
            "Epoch 2/10, Validation Accuracy: 96.07%, Avg. Bbox Loss: 0.0135\n",
            "Epoch 3/10, Validation Accuracy: 85.64%, Avg. Bbox Loss: 0.0113\n",
            "Epoch 4/10, Validation Accuracy: 96.88%, Avg. Bbox Loss: 0.0095\n",
            "Epoch 5/10, Validation Accuracy: 93.36%, Avg. Bbox Loss: 0.0112\n",
            "Epoch 6/10, Validation Accuracy: 95.66%, Avg. Bbox Loss: 0.0125\n",
            "Epoch 7/10, Validation Accuracy: 96.88%, Avg. Bbox Loss: 0.0108\n",
            "Epoch 8/10, Validation Accuracy: 93.90%, Avg. Bbox Loss: 0.0091\n",
            "Epoch 9/10, Validation Accuracy: 96.61%, Avg. Bbox Loss: 0.0078\n",
            "Epoch 10/10, Validation Accuracy: 80.89%, Avg. Bbox Loss: 0.0111\n"
          ]
        }
      ]
    }
  ]
}